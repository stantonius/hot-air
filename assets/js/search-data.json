{
  
    
        "post0": {
            "title": "fastai Sudoku - One Solution with Notes",
            "content": "Use a Sudoku puzzle to learn more about the fastai DataBlock, Datasets, DataLoaders and TfmdDL objects. . Why even bother with this? Jeremy&#39;s advice is to build models as quick as you can. However I find myself tripping at the first hurdle many times - the data preparation stage. It doesn&#39;t matter how many times I read about DataBlocks and DataLoaders, there is no substitute for actually using the libraries. So why not outline a game that requires you to build and solve the puzzle by using the same tools that you need to structure the data for a fastai Learner . This is a solution notebook for the fastai Sudoku notebook . !pip install py-sudoku from fastai.text.all import * from fastai.vision.all import * from sudoku import Sudoku from functools import wraps . Collecting py-sudoku Downloading py_sudoku-1.0.2-py3-none-any.whl (7.0 kB) Installing collected packages: py-sudoku Successfully installed py-sudoku-1.0.2 . class FastSudoku: &quot;&quot;&quot; Learn how to use the fastai DataBlock, Datasets, and DataLoaders transforms and callbacks by creating and solving a sudoku puzzle &quot;&quot;&quot; def __init__(self, difficulty: float = 0.25, data_dir: Path = Path(&quot;.&quot;)): self.puzzle = Sudoku(3).difficulty(difficulty) self.solved = self.puzzle.solve().board pd.DataFrame(self.puzzle.board).to_csv(data_dir/&quot;fastsudoku.csv&quot;, index=False) def __repr__(self): return f&quot;Puzzle created&quot; @staticmethod def np2list(x): return None if np.isnan(x) else int(x) def check(self, dls): &quot;&quot;&quot; Unpack the dataloaders output, convert to int and str &quot;&quot;&quot; holder = [] for dl in dls: holder+=dl self.preds = [list(map(self.np2list,j)) for j in [i[0] for i in holder]] print([self.solved[i] == x for i, x in enumerate(self.preds)]) if all(j for j in [self.solved[i] == x for i, x in enumerate(self.preds)]): print(&quot;Yes you are a fastai...and sudoku...whiz!&quot;) else: print(&quot;Try again!&quot;) . fs = FastSudoku(0.1) . fs.puzzle.show() . +-+-+-+ | 7 3 4 | 9 2 6 | 5 1 8 | | 1 8 9 | 3 4 5 | 2 6 7 | | 2 6 5 | 7 8 | 3 4 9 | +-+-+-+ | 5 7 3 | 6 9 | 4 2 1 | | 6 2 1 | 7 3 4 | 9 5 | | 8 | 2 5 1 | 7 3 | +-+-+-+ | 3 4 6 | 5 1 7 | 9 8 2 | | 8 1 2 | 6 9 3 | 5 | | 9 5 7 | 4 8 2 | 1 3 6 | +-+-+-+ . Challenge 1 - Datasets . Instructions: . Grab the fastsudoku.csv puzzle and create a DataBlock | You should try and use as many of the functions below as DataBlock arguments - not necessarily all, but as many as you can/wish. The point of this is not to be the most efficient or practical way of creating a DataBlock but rather to understand what each function argument does. | . Hints: . The y values are the row indices. They are not dependent variables as they normally are, but rather are a tool to help when processing batches (processed out of sequential order) | Tips: . Don&#39;t be afraid to comment out lines to see how the absence of functions changes the output | Use print statements | def get_items(a): df = pd.read_csv(a) df[&quot;y&quot;] = df.index.to_list() return df def get_x(a): return a.to_list()[:-1] def get_y(a): return a[&quot;y&quot;] ################# # Defined for you ################# def splitter(a): # In this exercise, we don&#39;t need train and validation sets # But never forget about them because theyre so important! return [list(range(9)),] . # check out lines XX in REPO - you&#39;ll see that fastai-delivered blocks are just &quot;holders&quot; for # transforms that end up being merged with transforms specified below dblock = DataBlock( get_items=get_items, get_x=get_x, get_y=get_y, splitter=splitter ) # why no batch or item_tfms? Because we are creating a Dataset # if you look https://github.com/fastai/fastai/blob/master/fastai/data/block.py # lines 142-160, you see that item and batch transforms are only ever called in the dataloaders method . Check format - note we only want the x values . dsets = dblock.datasets(&quot;fastsudoku.csv&quot;); . fs.puzzle.show() dsets.items . +-+-+-+ | 7 3 4 | 9 2 6 | 5 1 8 | | 1 8 9 | 3 4 5 | 2 6 7 | | 2 6 5 | 7 8 | 3 4 9 | +-+-+-+ | 5 7 3 | 6 9 | 4 2 1 | | 6 2 1 | 7 3 4 | 9 5 | | 8 | 2 5 1 | 7 3 | +-+-+-+ | 3 4 6 | 5 1 7 | 9 8 2 | | 8 1 2 | 6 9 3 | 5 | | 9 5 7 | 4 8 2 | 1 3 6 | +-+-+-+ . 0 1 2 3 4 5 6 7 8 y . 0 7.0 | 3.0 | 4 | 9.0 | 2 | 6 | 5.0 | 1 | 8.0 | 0 | . 1 1.0 | 8.0 | 9 | 3.0 | 4 | 5 | 2.0 | 6 | 7.0 | 1 | . 2 2.0 | 6.0 | 5 | NaN | 7 | 8 | 3.0 | 4 | 9.0 | 2 | . 3 5.0 | 7.0 | 3 | NaN | 6 | 9 | 4.0 | 2 | 1.0 | 3 | . 4 6.0 | 2.0 | 1 | 7.0 | 3 | 4 | NaN | 9 | 5.0 | 4 | . 5 NaN | NaN | 8 | 2.0 | 5 | 1 | NaN | 7 | 3.0 | 5 | . 6 3.0 | 4.0 | 6 | 5.0 | 1 | 7 | 9.0 | 8 | 2.0 | 6 | . 7 8.0 | 1.0 | 2 | 6.0 | 9 | 3 | NaN | 5 | NaN | 7 | . 8 9.0 | 5.0 | 7 | 4.0 | 8 | 2 | 1.0 | 3 | 6.0 | 8 | . Challenge 2 - DataLoader Sudoku . Instructions: . Use the DataLoaders callbacks to modify the Datasets you just created to solve the Sudoku board | Test your DataLoaders object against the puzzle, you can use the fs.check(dls) method | You should try and use as many of the callback functions below - not necessarily all, but as many as you can/wish. The point of this is not to be the most efficient or practical way of creating a DataLoaders but rather to understand what each function argument does. | . Tips: . Don&#39;t be afraid to comment out lines to see how the absence of functions changes the output | Use print statements | The Sudoku puzzle is below: . fs.puzzle.board = saved_board . fs.puzzle.show() . +-+-+-+ | 7 3 4 | 9 2 6 | 5 1 8 | | 1 8 9 | 3 4 5 | 2 6 7 | | 2 6 5 | 7 8 | 3 4 9 | +-+-+-+ | 5 7 3 | 6 9 | 4 2 1 | | 6 2 1 | 7 3 4 | 9 5 | | 8 | 2 5 1 | 7 3 | +-+-+-+ | 3 4 6 | 5 1 7 | 9 8 2 | | 8 1 2 | 6 9 3 | 5 | | 9 5 7 | 4 8 2 | 1 3 6 | +-+-+-+ . def s(p,r,c,v): &quot;&quot;&quot; Helper function that takes non-zero indexed row and column and inserts v into puzzle p &quot;&quot;&quot; for i in p: if i[1] == r-1: i[0][c-1] = v return p def before_iter(): &quot;&quot;&quot; No clue what this is for &quot;&quot;&quot; return a def after_item(a): &quot;&quot;&quot; Gets each item from the dataset &quot;&quot;&quot; return a def before_batch(a): &quot;&quot;&quot; Takes a list of items of length batch-size &quot;&quot;&quot; a = s(a,3,4,1) a = s(a,4,4,8) a = s(a,5,7,8) a = s(a,6,7,6) a = s(a,6,1,4) a = s(a,6,2,9) return a def after_iter(): pass def create_batches(a): return a def create_item(a): return np.array(a) def create_batch(a): &quot;&quot;&quot; Generally calls a collate function. If just returning a, it overrides any collation &quot;&quot;&quot; return a def after_batch(a): &quot;&quot;&quot; The actual batch &quot;&quot;&quot; a = s(a,8,9,4) a = s(a,8,7,7) return a . dls = TfmdDL( dsets, bs=2, # keep as 2 for this exercise # before_iter=before_iter, after_item=after_item, before_batch=before_batch, # after_iter=after_iter, # create_item=create_item, create_batch=create_batch, after_batch=after_batch, # create_batches=create_batches ) . This is an incomplete solution as the puzzle changes after each run . fs.check(dls) . [True, True, True, True, True, True, True, True, True] Yes you are a fastai...and sudoku...whiz! . Notes . Below are the notes and summaries that I picked up on as part of doing this Sudoku exercise. . ELI5 . General fastai . Everything before a fastai Learner is just data preparation to the correct format that a Pytorch model can interpret (in other words, pre-learner tasks are just ETL steps) | . DataLoaders . Output a list of tuples where for each item in the list, the first element of the tuple is a single x value (independent variable), and the second element of the tuple is the y value (the dependent variable) In creating a DataLoaders object, we apply transforms to the data either as we are fetching the item (called item_tfms) or after the batch is collated (called batch_tfms) | . | Note - when batched, all Xs and all Ys are stacked into a single typle | . | . TfmdDL . Also referred to as dl_type, this inherits all of the transforms and callbacks applied to the DataLoaders object and applies them to an iterable. | Under the hood, this is the major class that ultimately prepares the data for use in the Learner | . Collate . I never understood this word even though I saw it everywhere. fa_collate and the Pytorch default_collate are the actual functions that create a batch. If they aren&#39;t applied, each &quot;batch&quot; would just be one tupe, where the first argument is a single input and the second argument is a single target. When these functions are used, they &quot;Puts each data field into a tensor with outer dimension batch size&quot;. In other words, they stack items of batches together - one stack for inputs, one stack for targets | These functions are what are called if you do not specify the create_batch callback | . | . DataBlock . Prepackaged transforms for the most common types of data transformations in deep learning | . Transform . Converting the data (inputs/x and targets/y) into a format the computer understands and can perform matrix math on - tensors For deep learning, a transform directly or indirectly (via a Pipeline) converts a piece of data into a tensor | . | . ??DataLoaders . Init signature: DataLoaders(*loaders, path=&#39;.&#39;, device=None) Source: class DataLoaders(GetAttr): &#34;Basic wrapper around several `DataLoader`s.&#34; _default=&#39;train&#39; def __init__(self, *loaders, path=&#39;.&#39;, device=None): self.loaders,self.path = list(loaders),Path(path) if device is not None or hasattr(loaders[0],&#39;to&#39;): self.device = device def __getitem__(self, i): return self.loaders[i] def __len__(self): return len(self.loaders) def new_empty(self): loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders] return type(self)(*loaders, path=self.path, device=self.device) def _set(i, self, v): self.loaders[i] = v train ,valid = add_props(lambda i,x: x[i], _set) train_ds,valid_ds = add_props(lambda i,x: x[i].dataset) @property def device(self): return self._device @device.setter def device(self, d): for dl in self.loaders: dl.to(d) self._device = d def to(self, device): self.device = device return self def _add_tfms(self, tfms, event, dl_idx): &#34;Adds `tfms` to `event` on `dl`&#34; if(isinstance(dl_idx,str)): dl_idx = 0 if(dl_idx==&#39;train&#39;) else 1 dl_tfms = getattr(self[dl_idx], event) apply(dl_tfms.add, tfms) def add_tfms(self,tfms,event,loaders=None): &#34;Adds `tfms` to `events` on `loaders`&#34; if(loaders is None): loaders=range(len(self.loaders)) if not is_listy(loaders): loaders = listify(loaders) for loader in loaders: self._add_tfms(tfms,event,loader) def cuda(self): return self.to(device=default_device()) def cpu(self): return self.to(device=torch.device(&#39;cpu&#39;)) @classmethod def from_dsets(cls, *ds, path=&#39;.&#39;, bs=64, device=None, dl_type=TfmdDL, **kwargs): default = (True,) + (False,) * (len(ds)-1) defaults = {&#39;shuffle&#39;: default, &#39;drop_last&#39;: default} tfms = {k:tuple(Pipeline(kwargs[k]) for i in range_of(ds)) for k in _batch_tfms if k in kwargs} kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items() if k not in _batch_tfms}, tfms) kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)] return cls(*[dl_type(d, bs=bs, **k) for d,k in zip(ds, kwargs)], path=path, device=device) @classmethod def from_dblock(cls, dblock, source, path=&#39;.&#39;, bs=64, val_bs=None, shuffle=True, device=None, **kwargs): return dblock.dataloaders(source, path=path, bs=bs, val_bs=val_bs, shuffle=shuffle, device=device, **kwargs) _docs=dict(__getitem__=&#34;Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)&#34;, train=&#34;Training `DataLoader`&#34;, valid=&#34;Validation `DataLoader`&#34;, train_ds=&#34;Training `Dataset`&#34;, valid_ds=&#34;Validation `Dataset`&#34;, to=&#34;Use `device`&#34;, add_tfms=&#34;Add `tfms` to `loaders` for `event&#34;, cuda=&#34;Use the gpu if available&#34;, cpu=&#34;Use the cpu&#34;, new_empty=&#34;Create a new empty version of `self` with the same transforms&#34;, from_dblock=&#34;Create a dataloaders from a given `dblock`&#34;) File: /opt/conda/lib/python3.7/site-packages/fastai/data/core.py Type: type Subclasses: TextDataLoaders, ImageDataLoaders, SegmentationDataLoaders . DataBlock . fastai link: https://docs.fast.ai/data.block.html . A DataBlock is the quickest way to create a DataLoaders object; it is the most abstracted class from pure Pytorch. It should be used first when there is not much customization needed. . Remember - blocks are just pre-packaged transforms; they exist for the most common types of ML tasks (ie. CategoryBlock, ImageBlock) . Blocks . We just said blocks are pre-packaged transforms. What does this really mean? . Let&#39;s look at 2 common blocks: ImageBlock and CategoryBlock . CategoryBlock?? . Signature: CategoryBlock(vocab=None, sort=True, add_na=False) Source: def CategoryBlock(vocab=None, sort=True, add_na=False): &#34;`TransformBlock` for single-label categorical targets&#34; return TransformBlock(type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na)) File: /opt/conda/lib/python3.7/site-packages/fastai/data/block.py Type: function . ImageBlock?? . Signature: ImageBlock(cls=&lt;class &#39;fastai.vision.core.PILImage&#39;&gt;) Source: def ImageBlock(cls=PILImage): &#34;A `TransformBlock` for images of `cls`&#34; return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor) File: /opt/conda/lib/python3.7/site-packages/fastai/vision/data.py Type: function . This is interesting. Notice how there are no class methods - the only thing this class does is store transforms as attributes. . Now take a look back at ImageBlock and CategoryBlock - these are both functions and not classes (despite them using class formatting) . To recap: We know that blocks are to store transforms, and they all subclass TransformBlock. We have seen they are the first argument (generally) of the DataBlock API. . Note this term &quot;DataBlock API&quot; confused me for a while - in fact for a long time the term API in general caused confusion. To me, API in this sense just means a callable (function, class, url) that abstracts more complex code . DataBlock?? . Init signature: DataBlock( blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, *, get_items=None, splitter=None, get_y=None, get_x=None, ) Source: class DataBlock(): &#34;Generic container to quickly build `Datasets` and `DataLoaders`&#34; get_x=get_items=splitter=get_y = None blocks,dl_type = (TransformBlock,TransformBlock),TfmdDL _methods = &#39;get_items splitter get_y get_x&#39;.split() _msg = &#34;If you wanted to compose several transforms in your getter don&#39;t forget to wrap them in a `Pipeline`.&#34; def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, **kwargs): blocks = L(self.blocks if blocks is None else blocks) blocks = L(b() if callable(b) else b for b in blocks) self.type_tfms = blocks.attrgot(&#39;type_tfms&#39;, L()) self.default_item_tfms = _merge_tfms(*blocks.attrgot(&#39;item_tfms&#39;, L())) self.default_batch_tfms = _merge_tfms(*blocks.attrgot(&#39;batch_tfms&#39;, L())) for b in blocks: if getattr(b, &#39;dl_type&#39;, None) is not None: self.dl_type = b.dl_type if dl_type is not None: self.dl_type = dl_type self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders) self.dls_kwargs = merge(*blocks.attrgot(&#39;dls_kwargs&#39;, {})) self.n_inp = ifnone(n_inp, max(1, len(blocks)-1)) self.getters = ifnone(getters, [noop]*len(self.type_tfms)) if self.get_x: if len(L(self.get_x)) != self.n_inp: raise ValueError(f&#39;get_x contains {len(L(self.get_x))} functions, but must contain {self.n_inp} (one for each input) n{self._msg}&#39;) self.getters[:self.n_inp] = L(self.get_x) if self.get_y: n_targs = len(self.getters) - self.n_inp if len(L(self.get_y)) != n_targs: raise ValueError(f&#39;get_y contains {len(L(self.get_y))} functions, but must contain {n_targs} (one for each target) n{self._msg}&#39;) self.getters[self.n_inp:] = L(self.get_y) if kwargs: raise TypeError(f&#39;invalid keyword arguments: {&#34;, &#34;.join(kwargs.keys())}&#39;) self.new(item_tfms, batch_tfms) def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip( lambda g,tt: (g.fs if isinstance(g, Pipeline) else L(g)) + tt) def new(self, item_tfms=None, batch_tfms=None): self.item_tfms = _merge_tfms(self.default_item_tfms, item_tfms) self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms) return self @classmethod def from_columns(cls, blocks=None, getters=None, get_items=None, **kwargs): if getters is None: getters = L(ItemGetter(i) for i in range(2 if blocks is None else len(L(blocks)))) get_items = _zip if get_items is None else compose(get_items, _zip) return cls(blocks=blocks, getters=getters, get_items=get_items, **kwargs) def datasets(self, source, verbose=False): self.source = source ; pv(f&#34;Collecting items from {source}&#34;, verbose) items = (self.get_items or noop)(source) ; pv(f&#34;Found {len(items)} items&#34;, verbose) splits = (self.splitter or RandomSplitter())(items) pv(f&#34;{len(splits)} datasets of sizes {&#39;,&#39;.join([str(len(s)) for s in splits])}&#34;, verbose) return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose) def dataloaders(self, source, path=&#39;.&#39;, verbose=False, **kwargs): dsets = self.datasets(source, verbose=verbose) kwargs = {**self.dls_kwargs, **kwargs, &#39;verbose&#39;: verbose} return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs) _docs = dict(new=&#34;Create a new `DataBlock` with other `item_tfms` and `batch_tfms`&#34;, datasets=&#34;Create a `Datasets` object from `source`&#34;, dataloaders=&#34;Create a `DataLoaders` object from `source`&#34;) File: /opt/conda/lib/python3.7/site-packages/fastai/data/block.py Type: type Subclasses: . The L() object have the attrgot method with is how fastai can extract class atributes from a TransformBlock. This took me forever to understand . dblock = DataBlock( get_items=split_xy, # receives the entire iterable or path; used to fetch/parse the data and return an iterable get_x=get_x, # get_x and get_y both receive a single item from the output of the get_items function get_y=get_y, # get_x and get_y must return a tensor, array, list, dict item_tfms=item_tfms, batch_tfms=batch_tfms, ) . Next Steps . If I can find proper definitions for each of these functions, then I would like to compare someone&#39;s response to these definitions and use a model to see how similar their explanations are .",
            "url": "https://stantonius.github.io/home/fastai/2022/05/20/fastai_sudoku_notes.html",
            "relUrl": "/fastai/2022/05/20/fastai_sudoku_notes.html",
            "date": " • May 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "fastai Sudoku",
            "content": "Use a Sudoku puzzle to learn more about the fastai DataBlock, Datasets, DataLoaders and TfmdDL objects. . Why even bother with this? Jeremy&#39;s advice is to build models as quick as you can. However I find myself tripping at the first hurdle many times - the data preparation stage. It doesn&#39;t matter how many times I read about DataBlocks and DataLoaders, there is no substitute for actually using the libraries. So why not outline a game that requires you to build and solve the puzzle by using the same tools that you need to structure the data for a fastai Learner . !pip install py-sudoku . Requirement already satisfied: py-sudoku in /opt/conda/lib/python3.7/site-packages (1.0.2) . from fastai.text.all import * from fastai.vision.all import * from sudoku import Sudoku from functools import wraps . Challenge Overview . There are two parts to this challenge. The first is to create the Sudoku board via the DataBlock API. The second is to use the DataLoaders callbacks to actually solve the sudoku puzzle. . Questions I hope to answer along the way: . What is the purpose of all of these classes? | What is a DataBlock? What are the blocks within a DataBlock? | What is the difference between a Datasets object and a DataLoaders object? | What is a DataLoaders object vs a TfmdDL object? | What callbacks are called in what order? | When would I use each type of callback? | What is fa_collate and which callback is it used in by default? | . Setup . class FastSudoku: &quot;&quot;&quot; Learn how to use the fastai DataBlock, Datasets, and DataLoaders transforms and callbacks by creating and solving a sudoku puzzle &quot;&quot;&quot; def __init__(self, difficulty: float = 0.25, data_dir: Path = Path(&quot;.&quot;)): self.puzzle = Sudoku(3).difficulty(difficulty) self.solved = self.puzzle.solve().board pd.DataFrame(self.puzzle.board).to_csv(data_dir/&quot;fastsudoku.csv&quot;, index=False) def __repr__(self): return f&quot;Puzzle created&quot; @staticmethod def np2list(x): return None if np.isnan(x) else int(x) def check(self, dls): &quot;&quot;&quot; Unpack the dataloaders output, convert to int and str &quot;&quot;&quot; holder = [] for dl in dls: holder+=dl self.preds = [list(map(self.np2list,j)) for j in [i[0] for i in holder]] print([self.solved[i] == x for i, x in enumerate(self.preds)]) if not any([self.solved[i] == x for i, x in enumerate(self.preds)]): print(&quot;Yes you are a fastai...and sudoku...whiz!&quot;) else: print(&quot;Try again!&quot;) . fs = FastSudoku(difficulty=0.1) . Challenge 1 - Datasets . Instructions: . Grab the fastsudoku.csv puzzle and create a DataBlock | You should try and use as many of the functions below as DataBlock arguments - not necessarily all, but as many as you can/wish. The point of this is not to be the most efficient or practical way of creating a DataBlock but rather to understand what each function argument does. | . Hints: . The y values are the row indices. They are not dependent variables as they normally are, but rather are a tool to help when processing batches (processed out of sequential order) | Tips: . Don&#39;t be afraid to comment out lines to see how the absence of functions changes the output | Use print statements | class DataBlockNotes: &quot;&quot;&quot; Class to hold and access all function comments to make a summary if desired &quot;&quot;&quot; pass @patch_to(DataBlockNotes) def get_items(): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; pass @patch_to(DataBlockNotes) def get_x(): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; pass @patch_to(DataBlockNotes) def get_y(): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; pass @patch_to(DataBlockNotes) def item_tfms(): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; pass @patch_to(DataBlockNotes) def batch_tfms(): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; pass # predefined def splitter(): # In this exercise, we don&#39;t need train and validation sets # But never forget about them because theyre so important! return [list(range(10)),] . Create the DataBlock and Datasets . dblock = DataBlock( get_items=get_items, get_x=get_x, get_y=get_y, # batch_tfms=batch_tfms, # item_tfms=item_tfms splitter=splitter ) dsets = dblock.datasets(&quot;fastsudoku.csv&quot;) . fs.puzzle.show() dsets.items . Challenge 2 - DataLoader Sudoku . Instructions: . Use the DataLoaders callbacks to modify the Datasets you just created to solve the Sudoku board | Test your DataLoaders object against the puzzle, you can use the fs.check(dls) method | You should try and use as many of the callback functions below - not necessarily all, but as many as you can/wish. The point of this is not to be the most efficient or practical way of creating a DataLoaders but rather to understand what each function argument does. | . Tips: . Don&#39;t be afraid to comment out lines to see how the absence of functions changes the output | Use print statements | The Sudoku puzzle is below: . fs.puzzle.show() . Functions to use . class TfmdDLNotes: &quot;&quot;&quot; Class to hold and access all function comments to make a summary if desired &quot;&quot;&quot; pass @patch_to(TfmdDLNotes) def before_iter(): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; return a @patch_to(TfmdDLNotes) def after_item(a): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; return a @patch_to(TfmdDLNotes) def before_batch(a): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; return a @patch_to(TfmdDLNotes) def after_iter(): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; pass @patch_to(TfmdDLNotes) def create_batches(a): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; return a @patch_to(TfmdDLNotes) def create_item(a): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; return a @patch_to(TfmdDLNotes) def create_batch(a): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; return a @patch_to(TfmdDLNotes) def after_batch(a): &quot;&quot;&quot; TODO: Your summary of what this function does &quot;&quot;&quot; return a . dls = TfmdDL( dsets, bs=2, # keep as 2 for this exercise; can be any number below 10 before_iter=before_iter, after_item=after_item, before_batch=before_batch, after_iter=after_iter, create_item=create_item, create_batch=create_batch, after_batch=after_batch, create_batches=create_batches ) . fs.check(dls) . Your Notes . for k,v in DataBlockNotes.__dict__.items(): if &quot;__&quot; not in k: print({k: rm_useless_spaces(v.__doc__).strip()}) . {&#39;get_items&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;get_x&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;get_y&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;item_tfms&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;batch_tfms&#39;: &#39;TODO: Your summary of what this function does&#39;} . for k,v in TfmdDLNotes.__dict__.items(): if &quot;__&quot; not in k: print({k: rm_useless_spaces(v.__doc__).strip()}) . {&#39;before_iter&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;after_item&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;before_batch&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;after_iter&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;create_batches&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;create_item&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;create_batch&#39;: &#39;TODO: Your summary of what this function does&#39;} {&#39;after_batch&#39;: &#39;TODO: Your summary of what this function does&#39;} .",
            "url": "https://stantonius.github.io/home/fastai/2022/05/20/fastai_sudoku.html",
            "relUrl": "/fastai/2022/05/20/fastai_sudoku.html",
            "date": " • May 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "fastai 2022 - Week 1 - Private(-ish) Computer Vision",
            "content": "Private CV - Mini project outline . Can CV be &quot;private&quot;*? Or asked another way, how much &quot;detail&quot; do we need to give a model in order to get the output we want (in this case, a simple multi-class classification)? . If we can show, for example, that an 8-bit-style image is sufficient to achieve out outcome, can we use extremely small models that use significantly less energy (and are thus better suited for battery-operated smart home devices) while also being respectful of privacy? . Whatever the outcome, the answers to this problem will help me discect DL models in far greater detail than I have have done in the past. . * In this definition borrowed from OpenMined, privacy means plausible deniability. In the case of a smart home camera, this would mean that the model would detect that someone was present in the room but a nefarious actor could never generate accurate high-definition images of the actual person/environment if they were to get their hands on the images/feed. . Background . Why am I refering to smart home cameras and CV privacy? A little while back I was into building a smart home myself (using Raspberry Pis, ESP32s, sensors, etc.). I had this idea of having sensors to turn on/off these cool LED lights in my office when depending on my proximity to them (is this lazy or cool? Both?). To spare you the details, it didn&#39;t really work using Bluetooth from your phone as a location beacon (it was super rewarding though). . The next iteration of the experiment was to use some advanced presence-detection sensors (some even claim they can detect presence of a person from their breathing while lying completely still). However these sensors are expensive and require an additional level of complexity that I don&#39;t have the time for. . Alternatively, it did cross my mind to have a camera instead of an expensive sensor - I have many of those lying around and the technology is defnitely available; I am familiar enough with DL that I could build a responsive system with relative ease. The only caveat to this is I am not 100% confident in my smart home security - and I have this nightmare of some hacker hijacking the video feed (although, my life is currently so uneventful that for anyone watching it would be like a quickly cancelled Truman Show spinoff). I came to the conclusion that the only way I would ever be comfortable with an indoor camera would be if I had a physical/mechanical filter on the camera that would limit the detail of the image. . So my project for the fastai course is to see if I can build an indoor camera system that a) performs simple smart home tasks and b) I am comfortable with. . For the record, it is entirely possible this logic is flawed. For example, I know that GANs for example are incredible at generating an image from a simple prompt - however these are all gueses by the GAN model and never the real thing. . Next Steps . Have someone poke holes in this theory/approach and conduct more research on private CV | Make sure that the human training data is representitive of the broader population (look into some recent papers on image similarity) | Try using smaller models. Especially if we use the downsampling method, we don&#39;t need nearly the same pixel-by-pixel asessment (because most pixels are identical to their neighbour in a downsampled image, so why process them all?) I also recollect that it is the middle layers that recognize general patters, so I expect that most of these models pretrained on fine, high-definition features have too many layers for this specific task | . | Incorporate movement (ie. a GIF) as one of my theories is that for tricky downsampled images, the pixel change in relation to eachother as an object moves may improve the system accuracy. Since this was born out of the smart home example, there will usually be movement anyway. | Model surgery to identify the layers that are needed vs the ones that can be removed (for example the &quot;deeper&quot; layers may not be needed as we don&#39;t care about very fine detail of objects) | Can we add new features easy with a smaller model (ie. one/few-shot learning) to make this system bespoke for individuals? (ie. distinguish between a child and an adult to generate different response) | Optimize the transforms - right now just using the basic aug_transforms (which may be optimal - just need to try more) | Baseline Model . Set a benchmark in a couple of hours that we can look to improve in the coming weeks . Setup . %%capture !apt-get install ffmpeg libsm6 libxext6 -y !pip install -U fastai !pip install -U wandb !pip install timm . from fastcore.all import * import time from fastai.vision.all import * import wandb import albumentations as A from fastai.callback.wandb import * import timm . Get Images . def search_images(term, max_images=200): url = &#39;https://duckduckgo.com/&#39; res = urlread(url,data={&#39;q&#39;:term}) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;, res) requestUrl = url + &#39;i.js&#39; params = dict(l=&#39;us-en&#39;, o=&#39;json&#39;, q=term, vqd=searchObj.group(1), f=&#39;,,,&#39;, p=&#39;1&#39;, v7exp=&#39;a&#39;) urls,data = set(),{&#39;next&#39;:1} while len(urls)&lt;max_images and &#39;next&#39; in data: data = urljson(requestUrl,data=params) urls.update(L(data[&#39;results&#39;]).itemgot(&#39;image&#39;)) requestUrl = url + data[&#39;next&#39;] time.sleep(0.2) return L(urls)[:max_images] . urls = search_images(&#39;person&#39;, max_images=1) urls[0] . &#39;https://dm0qx8t0i9gc9.cloudfront.net/thumbnails/video/Vd3bj2jPe/videoblocks-angry-man-call-phone-at-kitchen-frustrated-person-talking-mobile-phone-at-home-annoyed-businessman-speaking-cellphone-upset-guy-arguing-in-phone-conversation-nervous-hipster-working-at-home_smnn9ns9v_thumbnail-1080_01.png&#39; . from fastdownload import download_url dest = &#39;bird.jpg&#39; download_url(urls[0], dest, show_progress=False) im = Image.open(dest) im.to_thumb(256,256) . searches = &quot;person&quot;,&#39;dog&#39;,&#39;cat&#39; path = Path(&#39;person_dog_cat&#39;) # for o in searches: # dest = (path/o) # dest.mkdir(exist_ok=True, parents=True) # download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) # resize_images(path/o, max_size=400, dest=path/o) . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . 0 . Transforms . Use the albumentations library to perform two different types of image filters (to downscale the image quality): . Blur | Downscale | . In order to get the right parameters for these transforms, I used the Streamlit app that allows your to see the output. . First we need to create a custom Transform . import albumentations as A class AlbTransform(Transform): def __init__(self, aug): self.aug = aug def encodes(self, img: PILImage): aug_img = self.aug(image=np.array(img))[&#39;image&#39;] return PILImage.create(aug_img) # tfm = AlbTransform(A.Blur(blur_limit=(20,20), p=1)) # a,b = tfm((img, &quot;person&quot;)) # show_image(a, title=b); . blur = AlbTransform(A.Blur(blur_limit=(20,20), p=1)) downscale = AlbTransform(A.Downscale(always_apply=True, scale_min=0.1, scale_max=0.1, interpolation=3)) . Blur . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[blur, Resize(224, method=&#39;squish&#39;)], batch_tfms=aug_transforms() ).dataloaders(path) dls.show_batch(max_n=6) . We need to make sure that these transforms are applied to the validation data . dls[1].show_batch() . We can try a couple of models - see appendix section on how we find the tiny models . blur_learn = vision_learner(dls, &quot;deit_tiny_patch16_224&quot;, metrics=error_rate) . blur_learn.fine_tune(10) . epoch train_loss valid_loss error_rate time . 0 | 1.643323 | 0.801733 | 0.292035 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 1.116338 | 0.669186 | 0.230089 | 00:04 | . 1 | 0.992832 | 0.580699 | 0.194690 | 00:04 | . 2 | 0.928959 | 0.522768 | 0.159292 | 00:04 | . 3 | 0.814803 | 0.492130 | 0.159292 | 00:04 | . 4 | 0.723447 | 0.476217 | 0.159292 | 00:04 | . 5 | 0.649331 | 0.464862 | 0.150442 | 00:04 | . 6 | 0.585791 | 0.452028 | 0.150442 | 00:04 | . 7 | 0.532627 | 0.435493 | 0.150442 | 00:04 | . 8 | 0.495010 | 0.430052 | 0.141593 | 00:04 | . 9 | 0.465393 | 0.428942 | 0.141593 | 00:04 | . blur_learn.unfreeze() blur_learn.freeze_to(-2) . blur_learn.fit_one_cycle(10, lr_max=1e-4) . epoch train_loss valid_loss error_rate time . 0 | 0.294479 | 0.439127 | 0.115044 | 00:04 | . 1 | 0.299258 | 0.602763 | 0.185841 | 00:04 | . 2 | 0.319252 | 0.608905 | 0.230089 | 00:04 | . 3 | 0.300808 | 0.423359 | 0.150442 | 00:04 | . 4 | 0.279470 | 0.491656 | 0.159292 | 00:04 | . 5 | 0.246826 | 0.631488 | 0.185841 | 00:04 | . 6 | 0.228636 | 0.470130 | 0.115044 | 00:04 | . 7 | 0.205746 | 0.400601 | 0.132743 | 00:04 | . 8 | 0.183016 | 0.377757 | 0.123894 | 00:04 | . 9 | 0.164164 | 0.376342 | 0.115044 | 00:04 | . blur_learn.unfreeze() # blur_learn.freeze_to(-2) . blur_learn.fit_one_cycle(10, lr_max=1e-4) . epoch train_loss valid_loss error_rate time . 0 | 0.042513 | 0.419177 | 0.132743 | 00:04 | . 1 | 0.068904 | 0.559260 | 0.168142 | 00:04 | . 2 | 0.098786 | 0.857745 | 0.230089 | 00:04 | . 3 | 0.127667 | 0.581839 | 0.168142 | 00:04 | . 4 | 0.151713 | 0.319088 | 0.106195 | 00:04 | . 5 | 0.142025 | 0.435182 | 0.159292 | 00:04 | . 6 | 0.125716 | 0.417557 | 0.150442 | 00:04 | . 7 | 0.114356 | 0.490999 | 0.168142 | 00:04 | . 8 | 0.105546 | 0.499440 | 0.141593 | 00:04 | . 9 | 0.093576 | 0.483677 | 0.132743 | 00:04 | . blur_learn.fit_one_cycle(15, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.042361 | 0.481615 | 0.123894 | 00:04 | . 1 | 0.042992 | 0.474748 | 0.132743 | 00:04 | . 2 | 0.039247 | 0.456245 | 0.115044 | 00:04 | . 3 | 0.039396 | 0.447323 | 0.106195 | 00:04 | . 4 | 0.033891 | 0.467175 | 0.106195 | 00:04 | . 5 | 0.029939 | 0.469003 | 0.106195 | 00:04 | . 6 | 0.027763 | 0.469526 | 0.115044 | 00:04 | . 7 | 0.026574 | 0.479521 | 0.115044 | 00:04 | . 8 | 0.026066 | 0.491594 | 0.115044 | 00:04 | . 9 | 0.025001 | 0.493386 | 0.115044 | 00:04 | . 10 | 0.026526 | 0.504623 | 0.123894 | 00:04 | . 11 | 0.026841 | 0.502367 | 0.115044 | 00:04 | . 12 | 0.025512 | 0.502861 | 0.115044 | 00:04 | . 13 | 0.024459 | 0.499274 | 0.115044 | 00:04 | . 14 | 0.022739 | 0.495793 | 0.115044 | 00:04 | . Downscale . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[downscale, Resize(224, method=&#39;squish&#39;)], batch_tfms=aug_transforms() ).dataloaders(path) dls.show_batch(max_n=6) . dls[1].show_batch() . downscale_learn = vision_learner(dls, &quot;deit_tiny_patch16_224&quot;, metrics=error_rate) . downscale_learn.fine_tune(10, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.843037 | 1.091311 | 0.548673 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 1.467566 | 0.950696 | 0.486726 | 00:04 | . 1 | 1.323597 | 0.858683 | 0.380531 | 00:04 | . 2 | 1.239593 | 0.789310 | 0.327434 | 00:05 | . 3 | 1.176678 | 0.810763 | 0.318584 | 00:04 | . 4 | 1.119942 | 0.682093 | 0.283186 | 00:04 | . 5 | 1.037753 | 0.640808 | 0.274336 | 00:04 | . 6 | 0.954163 | 0.627087 | 0.265487 | 00:04 | . 7 | 0.879154 | 0.645445 | 0.265487 | 00:04 | . 8 | 0.835567 | 0.634394 | 0.274336 | 00:04 | . 9 | 0.802554 | 0.647319 | 0.265487 | 00:04 | . downscale_learn.unfreeze() downscale_learn.freeze_to(-2) . downscale_learn.fit_one_cycle(10, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.543624 | 0.645094 | 0.265487 | 00:04 | . 1 | 0.549186 | 0.633144 | 0.256637 | 00:04 | . 2 | 0.603437 | 0.578774 | 0.238938 | 00:04 | . 3 | 0.577562 | 0.510453 | 0.238938 | 00:04 | . 4 | 0.571037 | 0.481142 | 0.212389 | 00:04 | . 5 | 0.554202 | 0.479897 | 0.203540 | 00:05 | . 6 | 0.541147 | 0.490987 | 0.194690 | 00:04 | . 7 | 0.525918 | 0.486609 | 0.203540 | 00:04 | . 8 | 0.510616 | 0.484697 | 0.203540 | 00:04 | . 9 | 0.502333 | 0.483174 | 0.203540 | 00:04 | . downscale_learn.unfreeze() downscale_learn.freeze_to(-4) . downscale_learn.fit_one_cycle(10, lr_max=5e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.442562 | 0.451516 | 0.176991 | 00:04 | . 1 | 0.449721 | 0.435416 | 0.185841 | 00:04 | . 2 | 0.422973 | 0.436478 | 0.194690 | 00:04 | . 3 | 0.399138 | 0.379390 | 0.168142 | 00:04 | . 4 | 0.372680 | 0.362092 | 0.176991 | 00:04 | . 5 | 0.355311 | 0.353536 | 0.132743 | 00:04 | . 6 | 0.326759 | 0.402880 | 0.132743 | 00:04 | . 7 | 0.303226 | 0.357993 | 0.159292 | 00:04 | . 8 | 0.280136 | 0.344044 | 0.141593 | 00:04 | . 9 | 0.257645 | 0.346915 | 0.141593 | 00:04 | . Appendix . Which pretrained model is the smallest? . def learner_sz(learner): num_params = 0 for layer in learner.model.parameters(): num_params += layer.flatten().shape[0] return num_params . learner_sz(blur_learn) . 6369316 . model_path = Path(&quot;/root/.cache/torch/hub/checkpoints/&quot;) . models = [] for model in timm.list_models(&quot;*tiny*&quot;, pretrained=True): learner = vision_learner(dls, model, metrics=error_rate) models.append({&quot;model&quot;: model, &quot;sz&quot;: learner_sz(learner)}) try: next(model_path.glob(f&quot;{model}*&quot;)).unlink() except: continue . Downloading: &#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_lite_tiny-461b07a7.pth&#34; to /root/.cache/torch/hub/checkpoints/coat_lite_tiny-461b07a7.pth Downloading: &#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_tiny-473c2a20.pth&#34; to /root/.cache/torch/hub/checkpoints/coat_tiny-473c2a20.pth Downloading: &#34;https://dl.fbaipublicfiles.com/convit/convit_tiny.pth&#34; to /root/.cache/torch/hub/checkpoints/convit_tiny.pth Downloading: &#34;https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth&#34; to /root/.cache/torch/hub/checkpoints/convnext_tiny_1k_224_ema.pth Downloading: &#34;https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth&#34; to /root/.cache/torch/hub/checkpoints/deit_tiny_distilled_patch16_224-b40b3cf7.pth Downloading: &#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/jx_nest_tiny-e3428fb9.pth&#34; to /root/.cache/torch/hub/checkpoints/jx_nest_tiny-e3428fb9.pth Downloading: &#34;https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth&#34; to /root/.cache/torch/hub/checkpoints/swin_tiny_patch4_window7_224.pth Downloading: &#34;https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_a.pth&#34; to /root/.cache/torch/hub/checkpoints/tinynet_a.pth Downloading: &#34;https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_b.pth&#34; to /root/.cache/torch/hub/checkpoints/tinynet_b.pth Downloading: &#34;https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_c.pth&#34; to /root/.cache/torch/hub/checkpoints/tinynet_c.pth Downloading: &#34;https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_d.pth&#34; to /root/.cache/torch/hub/checkpoints/tinynet_d.pth Downloading: &#34;https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_e.pth&#34; to /root/.cache/torch/hub/checkpoints/tinynet_e.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_12_p8_224.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_12_p8_224_dist.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_384_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_12_p8_384_dist.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_12_p16_224.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_12_p16_224_dist.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_384_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_12_p16_384_dist.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_24_p8_224.pth /opt/conda/lib/python3.7/site-packages/torch/serialization.py:845: RuntimeWarning: coroutine &#39;get_all_models&#39; was never awaited storage = zip_file.get_storage_from_record(name, size, dtype).storage() RuntimeWarning: Enable tracemalloc to get the object allocation traceback Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_24_p8_224_dist.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_384_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_24_p8_384_dist.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_24_p16_224.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_24_p16_224_dist.pth Downloading: &#34;https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_384_dist.pth&#34; to /root/.cache/torch/hub/checkpoints/xcit_tiny_24_p16_384_dist.pth . models_df = pd.DataFrame(models) . models_df.sort_values(&quot;sz&quot;).to_csv(&quot;tiny_models.csv&quot;) . models_df.sort_values(&quot;sz&quot;) .",
            "url": "https://stantonius.github.io/home/fastai/2022/05/02/fastai2022-wk1-private-cv.html",
            "relUrl": "/fastai/2022/05/02/fastai2022-wk1-private-cv.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Creating a Microservice Package Repo with Google Artifact Registry and Cloud Build",
            "content": "Creating a Microservice Package Repo with Google Artifact Registry and Cloud Build . Quick Guide to Python Package Management for Microservices with Google Artifact Registry . Straight to what will help people the most - code with as little text as possible. Longer discussion below. . Assumes: You have generated a wheel file for package distribution | Cloud Build used for CI/CD | Authenticated to correct GCP project | . | . 1. Create Repository . gcloud artifacts repositories create REPOSITORY_NAME --repository-format=python --location=REGION --description=&quot;Repo description&quot; . To make your life easier, set defaults for repository and region: . gcloud config set artifacts/repository REPOSITORY_NAME gcloud config set artifacts/location REGION . 2. Authenticate with Google Artifact Registry . Required if not running in a GCP-managed container service like Cloud Build (ie. developing and testing locally). . Standard Authentication . As per the offical docs, you must update .pypirc and pip.conf to utilize keyring authentication. Let GCP tell you what to include by entering the following in the command line: . # if you have set defaults for repository, region, and project gcloud artifacts print-settings python # if defaults not set gcloud artifacts print-settings python --project=PROJECT --repository=REPOSITORY --location=LOCATION . If you’ve never uploaded a package to Pypi or similar before, you probably won’t have either .pypirc or pip.conf on your machine, so you must create them. . Developing on a Windows machine using a virtual environment, this is where I created them . C: Users user .pypirc C: Users user .virtualenvs VIRTUALENV_NAME pip.conf . Warning: I learnt the hard way that creating the pip.conf file globally is a horrible idea (as it checks the private repo for each package regardless of which project you are working on). Save yourself the headache and create this file in a virtual environment (you have no choice but to install .pypirc globally in $HOME) . If you do not already have twine and the GCP library for keychain validation, install both via the command below: . pip install twine keyrings.google-artifactregistry-auth . The offical docs recommend you run keyring --list-backends to check keyring works. They have a screenshot of what the output looks like. Don’t be concerned with the order of the output - it might be that the order matters (and why the Mac didn’t work), but unless you’re well versed in keychain then you can ignore this. . 3. Upload the Package to the Repository . In the package directory where the /dist subdirectory is (that contains the wheel), run the twine command below: . twine upload --repository-url https://REGION-python.pkg.dev/PROJECT_ID/REPOSITORY_NAME/ dist/* . Note that Artifact Registry does not allow package uploads of the same version, so if you get a HTTPError: 400 Bad Request from https://REGION-python.pkg.dev/PROJECT_ID/REPOSITORY_NAME Bad Request, you must either create a new package version or delete the current package via the console (there is probably a gcloud command to do that as well) . If this command returns the error EOFError: EOF when reading a line, GCP does not recognize this as an authenticated request. Thus you may need the alternative authentication as described in the section below. . Alternative Authentication (using M1 Mac) . Create a service account that exclusively is for accessing Artifacts (I assigned a newly created service account the role Artifact Registry Reader). Download its keys to a json file. . Unlike the above scenario, we need to print the repository configuration by including the path to the service account credentials. If you had already done this and had authentication issues, you will need to do again. . gcloud artifacts print-settings python --project=PROJECT --repository=REPOSITORY --location=LOCATION --json-key=KEY-FILE . For Mac, here is where I created them (I created these files at the user level, not within a virtual env - this was a mistake) . $HOME/.pypirc $HOME/.config/pip/pip.conf . See my note earlier - creating pip.conf globally is a bad idea. Do it only in virtual envs . Note that I had to create the /pip directory as well in .config/ . Now to upload the package to Artifact Registry, you must include your credentials as base64-encoded in the request. . How do you Base64-encode your credentials in a json file? You literally encode the whole file. In UNIX you do this via cat key.json | base64 . twine upload --repository-url https://_json_key_base64:XXXXXXXXXX@REGION-python.pkg.dev/PROJECT_ID/REPOSITORY_NAME dist/* . Notice the _json_key_base64 username and the : and @ separators, and also the credentials XXXXXX will be very long . If you are ever prompted for a username and password following this command, just hit Enter. If the credentials worked, you will still be asked for a username and password, but you leave them blank. . Another cheeky Authentication Method . Thanks to [this great post] by Lukas Karlsson, there is also another undocumented authentication method which uses the gcloud auth print-access-token command to get an OAuth2 access token. With this method, the upload URL becomes: . twine upload --repository-url https://oauth2accesstoken:XXXXXXXXX@REGION-python.pkg.dev/PROJECT_ID/REPOSITORY_NAME dist/* . Where XXXXXX is the output of the command gcloud auth print-access-token . One thing I noticed about this method was you don’t need any of the keyring/pip.conf/.pypic setup. You only need twine. . 4. Install a Package hosted in Artifact Registry . This comes back to the original purpose of all of this work - installing a private package multiple times into each of our microservices. The CI/CD tool used is Google’s Cloud Build. . There are two ways to install an Artifact Package. . Actively Providing Credentials . We can use the same credential methods we just discussed, only this time to download. You will see that nothing changes in the URLs except for the addition of /simple to the endpoint . Using a service account: . pip install --index-url https://https://_json_key_base64:XXXXXXXXX@REGION-python.pkg.dev/PROJECT_ID/REPOSITORY_NAME/simple/ PACKAGE_NAME . Using the OAuth2 method: . pip install --index-url https://https://oauth2accesstoken:XXXXXXXXX@REGION-python.pkg.dev/PROJECT_ID/REPOSITORY_NAME/simple/ PACKAGE_NAME . Building with Docker locally . If you want to use a Docker container and test locally, you will need to set the GOOGLE_APPLICATION_CREDENTIALS for the Docker container (thanks to Kenji Koshikawa’s post that made this very clear). In Kenji’s article, you can see in the Dockerfile config that they set this environment variable to the path to the service account credentials on the local machine (remember: the credentials are only needed during the build to install the package; they’re not needed to run the container after setup) . ENV GOOGLE_APPLICATION_CREDENTIALS &quot;key.json&quot; # ... RUN pip install keyrings.google-artifactregistry-auth RUN pip install --no-cache-dir -r requirements.txt . requirements.txt . The syntax for installing a package from Artifact Registry using requirements.txt is: . --extra-index-url https://REGION-python.pkg.dev/PROJECT_ID/REPOSITORY_NAME/simple sample-repository==0.0.1 . Using Cloud Build Default Credentials . Cloud Build tells us that its service account credentials are automatically authorized. In order to take advantage of this, you need to first run the Python Cloud Builder and use pip to install our private repo from Artifact Registry into /workspace. The gist of how this was set up is here. . If for some reason you need to use another persistent volume outside of /workspace, you would need to add an intermediary step to the Build to copy the installed package into /workspace. An example of this cloudbuild.yaml can be found here. . Initially I had tried to pip install the package artifact as part of the Dockerfile. This was a huge time sink until I realized that the Docker Builder container does not have access to the Cloud Build service account credentials. What this meant was if in the Dockerfile we tried to pip install our private package, we would get the same EOFError: EOF when reading a line error, telling us we needed to authenticate some way. . Also note that the directory name /workspace is not accessible inside the Dockerfile. Instead, the current directory is /workspace and is accessible via . . # ... WORKDIR /code COPY /workspace/installs /code # /workspace not accessible COPY ./installs /code # this will work - &#39;.&#39; is /workspace . Using Private Packages . You just saw we installed private packages into a non-standard directory. In order to use them, you need to point the PYTHONPATH environment variable to the directory that contains these packages. . ENV PYTHONPATH=&quot;/code/installs&quot; . For anyone who is unaware what PYTHONPATH does (like I was), it is another place that Python will look for packages. It is evaluated before the Python interpreter runs so that all packages and modules can be imported at any point when running Python. . The complete Dockerfile is below: . FROM python:3.9-slim-bullseye WORKDIR /code ENV PYTHONUNBUFFERED 1 ENV PYTHONPATH=&quot;/code/installs&quot; COPY . /code/ RUN pip install --upgrade -r /code/requirements.txt CMD [&quot;uvicorn&quot;, &quot;main:api&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8080&quot;] . Because we installed the package into the /workspace/, the COPY step brings it into the image and thus we set the PYTHONPATH to the directory . That is it. The package is now accessible and usable. . Discussion . A more thorough walkthrough of the observations and rationale. . Background . I am building a product using GCP-hosted services. The initial design consists of 3 microservices: a Slack chatbot, a processing engine, and a messaging platform (to message the user back) . . What I didn’t realize was one significant drawback of a microservice architecture - Don’t Repeat Yourself (DRY) problems. It seems obvious now - these microservice environments are completely isolated. If each of the services interact with the same database and use the same SQLAlchemy models, surely we would end up with functions and classes that could be used in all of the containers. . A quick search revealed that this is a very common dilemma and is a known drawback of microservice architecture. A summarization of the solutions to this problem are below: . DO repeat yourself - copy code to each of the containers (either manually or via a script or third-party service) | Abstracting the repeating code as a microservice itself. This was not ideal since what would need to be sent between services was tokens, which I wanted to avoid (despite being behind a VPC). Furthermore, this code I wanted in each of the containers was a mixture of functions and classes, not really designed as a standalone service. | Creating a private Python package that can be installed on each of the services (just like any other package) | GCP Artifact Registry . The Google Artifact Registry (GAR) stores the private Python package. You may have heard of Google Container Registry before - GAR is the evolution of that service, allowing storage and access of not only containers, but also code packages as well. . I decided to use this GCP service thinking it made sense given the integration of this service with tools I was using regularly: Cloud Build and Run. . Difference between a Python module, library, package can be found here . Why not Github or Pypi? . Fair question - both of those solutions would also work and might even be less of a headache. However I a) wanted to try GAR and b) using this service means one less set of credentials to manage (or so I thought*) since Cloud Build and Cloud Run are automatically authenticated for the Artifact , but GAR allows for everything to be managed by GCP (and because I am using Cloud Build and Cloud Run, there is no additional authentication that is needed) . * This is true if you are running or building containers within the GCP services Cloud Build and Cloud Run. However, developing locally is a different story . There are other useful features of GAR that are beyond the scope of this post as they benefit big teams (ie. the ability to restrict access to different repos based on GCP user roles) . Artifact Registry Authentication . This, plain and simple, was an unpleasant experience. This keyring approach that is recommended in the docs just never worked for me on the M1 Mac. And even using a Windows machine, it seemed like a lot of boilerplate (maybe this is standard for artifact/package management?). All I can say is this seemed like a lot more work than needed (and what it was worth, given ultimately I could have got the same features using a private Github repo). . Developing locally or in a Docker container required constant authentication or transfering of credentials to the image. The only way you can really take advantage of the Cloud Build automatic authorization is if you Cloud Build emulators locally, which I really dislike using because the responsiveness of these containers is so slow compared to local development using virtual environments or Docker containers. . Summary . Highlights: . Microservices that share code elements can use a package repo as one alternative to copy/pasting code in different locations; packages for repeat installation can be stored in Github, Pypi, or Google Artifact Registry | Storing a private Python package in Google Artifact Registry is difficult and not user friendly. However from the time spent on this, it does look like other artifact registries may also be similar. | The solution to develop locally depended on the machine used: with the M1 Mac, a dedicated service account and password authentication was the only solution that worked for me; on Windows I was able to avoid any additional steps other than those in the offical docs | . Is Artifact Registry worth the hassle, especially as a one-person band right now? No, probably not, especially given the Githib repo option. However I will probably continue to use it now that I have figured this out, so long as I can use bash scripts to automate all of this credential encoding when developing locally. I am glad however, that my intuition on how to manage microservice code distribution is aligned to how others have approached this common problem. . Helpful Resources . Excellent post from Kenji Koshikawa who also took this Artifact Python package approach to solve the microservice code copy problem | A Cloud Build Github Trigger solution by Lukas Karlsson. They have some cool scripts and also a helpful undocumented trick on how to authenticate in an easier way | .",
            "url": "https://stantonius.github.io/home/gcp/cloud%20build/artifact%20registry/microservices/2022/03/26/Microservices_GAR_CloudBuild.html",
            "relUrl": "/gcp/cloud%20build/artifact%20registry/microservices/2022/03/26/Microservices_GAR_CloudBuild.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Managing Geofence events with Riverpod",
            "content": "Managing Geofence events with Riverpod . As described in the blog Proximity-triggered LEDs, my Android phone is able to advertize as a beacon that can be used by other systems in the house to track its proximity. . However, given that I try to be very security conscious, one of the problems I came across is how to prevent the beacon from broadcasting away from our house. Additionally, restricting Flutter events only when in a specific location will also help with battery life. . I was able to use Riverpod state management library to turn on/off Bluetooth Low Energy advertising and MQTT connections when I enter/leave the zone around my house. . Code . The code for this app is a work in progress, but can be found here . Note this project used the awesome geofence_service plugin and a lot of the code is borrowed from the examples in this library. . Solution . The Riverpod code was integrated into the geofence.dart file to solution the following tasks: . Update a state when geofence status changes | Turn BLE and MQTT on if geofence status is “ENTER”, and turn them off if status is “LEAVE” | 1. Stream Changes &lt;&gt; State . Near the top of the geofence.dart file, you can see we have made 3 StreamControllers. . // example of one of the StreamController StreamController&lt;GeofenceStatus&gt; _geofenceStreamController = StreamController&lt;GeofenceStatus&gt;(); . This is where I got caught up. StreamController is not a Riverpod class, and yet I was looking for solutions to this state problem exclusively within Riverpod. However Riverpod is just an (excellent) tool to facilitate state management - it doesn’t have all the answers (it wasn’t designed to replace all base classes provided by Dart) . StreamController is effectively a container that holds the stream values. But on its own it is useless. We need to make a connection between when the phone signals there has been a geofence change and the StreamController. This is done in 2 parts: . Create a _onGeofenceStatusChanged() callback function that binds the geofence change to the StreamController sink. In this case we are only storing the GeofenceStatus, but we could store others (such as distance to geofence). | Future&lt;void&gt; _onGeofenceStatusChanged( Geofence geofence, GeofenceRadius geofenceRadius, GeofenceStatus geofenceStatus, Location location) async { print(&#39;geofence: $geofence&#39;); print(&#39;geofenceRadius: $geofenceRadius&#39;); print(&#39;geofenceStatus: ${geofenceStatus.toString()}&#39;); _geofenceStreamController.sink.add(geofenceStatus); //this is the binding } . Use the Riverpod StreamProvider class, to link any changes from the stream to any ref that is watching for changes | final geofenceStreamProvider = AutoDisposeStreamProvider&lt;GeofenceStatus&gt;((ref) { ref.onDispose(() { _geofenceStreamController.close(); }); // update another state object return _geofenceStreamController.stream; }); //.... class GeofenceDetails extends ConsumerWidget { const GeofenceDetails({Key? key}) : super(key: key); @override Widget build(BuildContext context, WidgetRef ref) { // below watches for any changes via the geofenceStreamProvider AsyncValue geofenceState = ref.watch(geofenceStreamProvider); // do something with the asyncvalue } } . We now have a system that can update state for every geofence status change. Next is to turn the beacon or MQTT connections on or off depending on the geofence status. . However we must tackle another item first - set the initial state when the app is loaded for the first time. . Set the initial Geofence Status . The way we handled this was to always assume the geofence status was GeofenceStatus.EXIT until the system overrides it. We used the callback system of the geofence_service plugin to set this status via the .start future. . WidgetsBinding.instance?.addPostFrameCallback((_) { // other callback bindings _geofenceService.start(_geofenceList).catchError(_onError).whenComplete( () =&gt; _geofenceStreamController.sink.add(GeofenceStatus.EXIT)); // when the start future is complete, set the geofence state to EXIT } . Now that we have an initial Geofence status that will update if we are within any of the geofences defined, we can set the BLE and MQTT properties. . 2. Turn on BLE and MQTT . It turns out that when the app is loaded, the BLE and MQTT states need to be set in an async manner. Because we cannot actually use an async when setting the app state, we needed another approach. . While not elegent, I settled on calling the ProviderContainer object to update the Provider state outside of a ConsumerWidget or another Provider. The ProviderContainer seems to be a wrapper class that houses all of the Providers and their states for the entire app. . // code from main.dart outlining the container for providers used in this app final container = ProviderContainer(); // permissions class final permissions = SmartHomePermissions(); void main() { runApp(UncontrolledProviderScope(container: container, child: MyApp())); } . The author of Riverpod suggests this is a possible (yet not recommended) solution to get access to the Provider state elsewhere in the UI (they go on to explain how you might approach this differently but I couldn’t wrap my head around this, and my solution below works). Regardless, the code below uses this provider container to read and set the BLE and MQTT states depending on the current Geofence status. . Future&lt;void&gt; _onGeofenceStatusChanged( Geofence geofence, GeofenceRadius geofenceRadius, GeofenceStatus geofenceStatus, Location location) async { // some print statements _geofenceStreamController.sink.add(geofenceStatus); if (geofenceStatus == GeofenceStatus.ENTER) { container.read(clientStateProvider.notifier).connect(); container.read(beaconStateProvider.notifier).bleOnSwitch(); } else if (geofenceStatus == GeofenceStatus.EXIT) { container.read(clientStateProvider.notifier).disconnect(); container.read(beaconStateProvider.notifier).bleKillSwitch(); } } . Conclusion . I was able to leverage the awesome Riverpod library to make a basic yet functioning Flutter app that dynamically connects with the ESP32 and the Raspberry Pi through BLE and MQTT, respectively. . Having spent a bit of time now with Flutter and state management, I have learnt to just follow what Remi (the creator of Riverpod) says when it comes to Flutter - and this Riverpod library, while quite opinionated, is ultimately very easy to use once you get the hang of it. . Flutter Tips . How do you test a Geofence in an emulator? . All we need to be able to do is manipulate the device’s GPS coordinates. Its as simple as selecting the three dot icon beside the emulator, then selecting Location. Here you can enter whatever coordinates you want (just go to Google Maps and find your home coordinates - can be found in the URL) and then hit Send. . How can you see the Lifecycle Events when testing? . It was extremely useful to print in the logs the Flutter lifecycle events when building this app. There is tonnes of documentation on how to set this up, however here is a brief overview for my own understanding. . First, the main Stateful Widget must add the mixin WidgetsBindingObserver and immediately add the observer: . class _MyHomePageState extends ConsumerState&lt;MyHomePage&gt; with WidgetsBindingObserver { @override void initState() { super.initState(); geofenceCallbacks(); // Add the observer WidgetsBinding.instance!.addObserver(this); permissions.requestPermission(); } // ... } . Once that is done, we can monitor lifecycle events by adding the following code. Note that we need to dispose as usual: . @override void dispose() { WidgetsBinding.instance!.removeObserver(this); container.read(beaconStateProvider.notifier).bleKillSwitch(); super.dispose(); } @override void didChangeAppLifecycleState(AppLifecycleState state) { super.didChangeAppLifecycleState(state); print(&quot;App Lifecycle State: ${state}&quot;); } .",
            "url": "https://stantonius.github.io/home/flutter/riverpod/2021/12/24/riverpod_geofence.html",
            "relUrl": "/flutter/riverpod/2021/12/24/riverpod_geofence.html",
            "date": " • Dec 24, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Proximity-triggered LEDs",
            "content": "Proximity-triggered LEDs . Use your phone as a beacon to turn on ESP32-controlled LEDs as you enter a room . This was effectively my Computer Science/IoT/Flutter Capstone project . Project Overview . We recently moved into a new house. I got the idea of making some ambient lights in the spare room from Andreas Speiss’ Raspberry Pi Server video - thanks Andreas and Google for the Youtube video recommendation. Its now 2.5 months later… . The tools used would be: . My Android Pixel 4 phone (as a Bluetooth beacon) A custom Flutter app acts as the beacon | . | A Raspberry Pi 4 8GB home server IOTStack running the necessary services | . | WS2812B LEDs (Neopixels) | ESP32 controller | . The key compoenents of this smart home system The idea: as I walked into the spare room where my workstation is, the ESP32 would recognize I was in the room and turn on the lights. When I left, they turn off. . A basic description of the system logic - when the beacon (phone) gets close enough to the sendor (ESP32), the lights should turn on I thought the above would be straightforward. I was wrong. Made me think about this tweet: . Almost every problem in my life can be traced to an unfounded optimism about how quickly I can get things done and how many things I can do at once. . &mdash; Sean J. Taylor (@seanjtaylor) October 7, 2021 Upon reflection, I had no real business trying this as my ack of computer science was exposed early on in the project - but a little stubornness and perserverence got me through it. . Current State of Project . This system works OK. Whilst I discuss potential improvements at the end, the two major fixes needed before this project can be considered a success are: . Prevent Android from randomly terminating apps running in the foreground | Add an RSSI processing filter on the RPi to improve proximity accuracy | . Architecture . The smart home architecture The system works as follows: . The Flutter app makes the Pixel phone a Bluetooth beacon (specifically, it uses BLE to periodically advertise itself as an iBeacon). In Bluetooth speak, this is called the server. A custom UUID is broadcast in the iBeacon message, which is what the Bluetooth client (the device that is scanning for iBeacons) uses as a filter to isolate only one specific device. | The ESP32 (Bluetooth client) runs Arduino sketches on both of its cores. One core uses the built-in antenna to continuously scan for a device with a specific UUID. As the diagram implies, the same antenna is used for both Wifi and Bluetooth communications, so the code flips between using the antenna for Bluetooth and Wifi. It is in this step that the ESP32 evaluates the proximity of the beacon using RSSI. | If the RSSI is above a certain threshold, the ESP32 sends a message via MQTT (over Wifi) to the Raspberry Pi server to indicate that a beacon is close. Conversely, when a beacon moves out of range (ie. the RSSI is below the threshold), the ESP32 sends a message indicating the beacon is not present. | IOTStack containers running on the Raspberry Pi make this the main server for the house. One of the containers is running the Mosquitto MQTT protocol and is subscribed to the specific topics which indicate if a beacon is close by or not. Another container is running NodeRed (a drag-and-drop UI for chaining together different actions depending on an input), which listens for the MQTT messages and triggers over MQTT back to the ESP32. | The ESP32 is simultaneously running an MQTT library that is subscribed to topics from the Raspberry Pi. The ESP32 interprets the incoming topic messages to either turn On or Off the Neopixel LED strip. | If the incoming topic contains an “On” message, the ESP32 sends the LED light program data through one of its GPIO pins which is connected to the LED strip. Conversely, an “Off” message stops the GPIO pin activity, and the LEDs shut off. | Code . The code to make the above system work has contains 3 distinct elements: . Arduino sketches (C++) for the ESP32 | Flutter app that contains the persistent beacon | IoTStack containers running on the Raspberry Pi | Code for ESP32 Beacon Sensor and Neopixels | Code for Smart Home App | Code for NodeRed flows | . Prototype Design . The initial plan was to have the LED strips around the base of the guest bed frame, which would have required ~5m of LEDs. Following some research, I came across two issues I wasn’t prepared to solve: . The high current needed for this type of setup was too much for a first project. The max current per LED (to produce a pure white light) is 60mA. With a light density of 60 LEDs/m, a total of 300 LEDs would need to be powered. At their brightest, this would equate to 18A (300 x 60mA) to power all the LEDs - this felt too dangerous for a first timer. | The 5m LED strip would need to be powered every ~1m to avoid voltage drop across the LED strip to avoid light dimming near the end of the strip. I did not have the enough wiring to power the LEDs this way. Note the type of LEDs used were WS2812B (aka Neopixels) which require 5V - as I discuss in [insert link to lessons blog], if I wanted to do bedframe LEDs in the future I might use 12V LEDs (since higher voltage experiences less voltage drop over the same distance compared to 5V) | | The next option was to mount the LEDs above the closet in the room, as there was a nice ledge to have the LEDs pointing down towards the floor. It also was ~1m long, so short enough to not require additional power input onto the strip. . The finished lights setup above the closet The design was such that the ESP32 would be over 2m away from the start of the LED strip - this was to avoid placing the homemade device inside the closet, which I was hesitant to do given this was a first time circuit build (I wanted the device visible to monitor any physical signs of system fault). I know this setup seems weird given we just discussed the impact of voltage drop on colour output - however I performed the following tests that confirmed this setup would work: . I used 18AWG wiring to connect the ESP32 and power source to the LEDs (the smaller the gauge, the wider the wiring diamter, the less voltage drop occurs). I then measured the voltage at the start and end of the 2.2m of wiring using a multimeter, and there was negligible drop. | I connected the 68 LED strip (just over 1m long) to the 2.2m wiring and didn’t notice any discolouration. | . A final point to mention is how I managed to power the LEDs safely. Two features were added: . An LED strip any longer than a handful of LEDs need to be powered from the mains (ie. using power from an outlet). Even though I had fixed the components to a protoboard, which can handle far more current than a regular breadboard, I decided to be extra precautious and add a power bypass so that the LEDs could theoretically consume the power needed directly from the mains. This was to avoid any chance that the protoboard would overheat from high amounts of current. | The current prototype setup. Notice the bypass red and white wiring that connects the mains to the LEDs I added a fuse to the LED power input with a max current of 2A. This was done out of an abundance of precaution, to avoid any short in the LED strip that could theoretically cause a fire. Note these were basic fuses I found on Amazon that I think are meant for automobile electronics - however the nice part of this setup is the fuse is easily replaceable so if the LEDs require more power, I can simply add a higher capacity fuse. | Image of generic auto fuses purchased from Amazon Centralized Server . In the initial design, the ESP32 was the centralized controller for managing the LED status. However this posed a problem - how do you turn off the LEDs if the sole logic evaluates whether the beacon is close to the sensor or not, and you are sitting in the room with your phone (ie. lying in the bed about to go to sleep)? . What we needed was a central server that would manage the LED status - this setup would allow different inputs to connect with the server and change the LEDs status (ie. the proximity beacon, as well as a light switch in the app, a Google Assistant integration, etc.). In this setup, the ESP32 just passes the status of the beacon to the central server (Raspberry Pi) via MQTT and listens for the LED status (also via MQTT). . 1. ESP32 MQTT setup . Although initially trying the PubSubClient Arduino library, I decided to try another library called AsyncMqttClient because a) it has been more recently maintained and b) it works in an async manner. Note I probably could have got the async working on the PubSubClient library, but I was struggling to wrap my head around it. . The AsyncMqttClient library has very clear documentation on how to set this up. However what I want to discuss is what tripped me up (and was also happening with the PubSubClient library - one of the reasons for switching) - the MQTT connection was dropping and therefore subscribed messages were not being received by the ESP32. . Wifi, BT, and the ESP32 antenna . After a lot of time researching, it finally clicked that since searching for beacons and delivering MQTT messages over Wifi happened immediately after eachother in the same loop, they were competing for the single antenna on the ESP32. . Here is an example of a forum that mentions exactly this - that Bluetooth and Wifi share a single antenna, and that when the device is sending/listening for a Bluetooth backet, it cannot send or receive a Wifi packet. . Therefore the change the eventually resulted in a consistent MQTT broker connection and subscribed message delivery was: . Adding delays to the code to allow the device time to send/receive and then free up the antenna. Note below, there are two 100ms delays following the BLE and the MQTT sequences. cpp | // https://github.com/stantonius/ESP32_smart_home_LEDs/blob/main/src/main.cpp . void codeForTaskRunBLEChecks(void *parameter) { for (;;) { if (pBLEScan-&gt;isScanning() == false) { // Start scan with: duration = 0 seconds(forever), no scan end callback, not a continuation of a previous scan. doBLEScans(pBLEScan); delay(100); } // logic for reducing false negatives if (deviceProximityHolder.sum() == 0) { if (isCloseVal) { isCloseVal = !isCloseVal; } mqttClient.publish(“BeaconProximity”, 0, false, “false”); } else { mqttClient.publish(“BeaconProximity”, 0, false, “true”); } delay(100); } } . 2. Closing the Bluetooth scan (which actively shuts down the Bluetooth use of the antenna) cpp // https://github.com/stantonius/ESP32_smart_home_LEDs/blob/main/src/ble.h void doBLEScans(NimBLEScan *pScan) { /* * Get results of scan; * Note that the scan results each trigger the callback when there are results regardless if its a beacon * Therefore we need to: * 1. Empty unordered_set to store the results for each scan * 2. Check the set to see if it includes a beacon * 3. If a beacon is found, add 1 to the holder vector; if not, add 0 to the holder vector * */ scanResults.clear(); NimBLEScanResults foundDevices = pScan-&gt;start(scanTime, false); if (scanResults.find(1) != scanResults.end()) { deviceProximityHolder.add(1); Serial.println(&quot;Beacon presence recorded&quot;); } else { deviceProximityHolder.add(0); Serial.println(&quot;No beacon presence recorded&quot;); }; Serial.print(&quot;Devices found: &quot;); Serial.println(foundDevices.getCount()); // stopping might be key as we want to free up the device antenna to use MQTT over Wifi pScan-&gt;stop(); pScan-&gt;clearResults(); // delete results fromBLEScan buffer to release memory } . 2. NodeRed Setup . While the flow that processes beacon proximity can be found here, I will just touch on the challenge I faced building this. . The NodeRed flow that listens for beacon proximity and switch commends from the Flutter app We needed logic that could be controlled by multiple inputs (beacon proximity and the app light switch) but that didn’t override eachother. For example, in the initial setup of the flow, the light switch could turn off the lights, but the very next scan saw the presence of the beacon and immediately turned the lights back on. As you will see if this flow, what ended up working was creating a state, whereby only when the state of one of the inputs was the same as the current state would the next state count. In other words, if we manually switched off the lights via the switch, only when the beacon is no longer present can it then turn the lights back on when it gets closer. . ESP32 Proximity Sketches . Without going into every element of the ESP32 Arduino sketches, I want to highlight two key elements of the proximity logic: . In the code above that defines the codeForTaskRunBLEChecks function, there is logic where we sum the two most recent scan events in the line if (deviceProximityHolder.sum() == 0). This evaluates whether the unordered_set deviceProximityHolder contains a found beacon from the prior 2 scans. If scan recognized the beacon, the lights turn off. This setup is to optimize for Type I errors (false positives) over Type II errors (false negatives). Obviously in an ideal world, we have no errors, however seeing as RSSI is an imperfect proximity measure, we have to pick a preferred scenario. In our case, we want the lights to turn on immediately when we enter the room, even if it means the lights sometimes turn on incorrectly. Conversely, we want the lights only to turn off if we are really sure the beacon is no longer present, and so we wait for two consecutive negatives before setting the LEDs to “Off” | As we mention above (and will mention in further detail later in this post), the RSSI threshold needs to be set experimentally and varies wildly depending on your local setup. Below is logic that the ESP32 uses to a) filter Bluetooth devices found on the scan using UUID, b) filter only if that device is close (below a certain threshold) and c) set the global variable beaconPresent if the beacon is close by. | . // check if the beacon is the one we set the service UUID for // this will exclude iTags for example if (oBeacon.getProximityUUID().toString() == SERVICE_UUID) { if (advertisedDevice-&gt;getRSSI() &gt; rssiThreshold) { beaconPresent = 1; } } . Conclusion . As I stated at the outset of this post, this project was incredible in terms of learning. However as a functioning system, it still need work on the issues mentioned above. . RSSI as a Proximity Measure . Others, and now myself, have shown that using RSSI as a proximity indicator is possible. However it is an imperfect science and should only be used when sub-metre precision is not needed. . The positioning of the beacon (server) and the receiver (client) also takes some tinkering. For example, I have the following observations regarding the position of the client: . Placing the client higher up in the room a) reduces false positives when my phone is on the floor below and b) reduces interference within the room (as most objects are on the floor in a room). The higher up the client, the more likely there is to be a direct line of sight between the beacon (phone) and the client (ESP32). | Related to the observation above, it turns out the guest room mattress must be incredibly dense as it causes a lot of interference. Therefore I could not place the client on the floor beside the bed, as my phone is most often on the other side of the bed where my desk is. | Wireless charging seems to weaken the BLE signal from my phone. | Play with threshold of RSSI cutoff and phone BLE power. You need to find a balance between false positives and false negatives for the former, and battery life vs signal strength for the latter. Related is the number of negatives recorded before the lights turn off themselves (currently 2 straight) | | As far as improving how the system evaluates RSSI in general, there is a great series of posts by Beaconzone that discuss improving the RSSI metric through processing/filtering to remove the noise. To me, this is the next option to try as it is the most cost effective. . Future Direction . Of course having done this project, there are many design changes and improvements that could be made to make this project even better. Some of them are outlined below: . Split out the sensor (Bluetooth client) and the LED controller onto two separate devices. I have a suspicion (for which I have zero evidence) that since the antennae is shared between Wifi and Bluetooth, that the Bluetooth client isn’t as strong a receiver as result. | Implement RSSI processing as mentioned above | Utilize (in part, or as a whole) new technologies: Devices with Bluetooth 5+ (the nrf52840 product sheet refers to “high-precision RSSI”) | Ultra wideband (UWB) which would replace using Bluetooth for proximity sensing with a much more accurate measurement Novelda has an UWB chip that can detect the presence of humans | . | Computer vision that detects presence (and even who) is in the room | . | . It is important to note that given the above potential solutions, they will come with risks. I have outlined my thoughts on the relative risks and advantages of these soltutions below: . My analysis of the benefits and risks of using different technical approaches to create a smart home LED system If anyone wants to collaborate on this project going forward, give me a shout! . Project References . https://github.com/filipsPL/cat-localizer | https://randomnerdtutorials.com/esp32-bluetooth-low-energy-ble-arduino-ide/ | .",
            "url": "https://stantonius.github.io/home/iot/microcomputing/flutter/2021/12/22/proximity_leds_main.html",
            "relUrl": "/iot/microcomputing/flutter/2021/12/22/proximity_leds_main.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Lessons and Notes - Proximity-triggered LEDs",
            "content": "Lessons and Notes - Proximity-triggered LEDs . A random collection of concepts that I took away from this project . TLDR . This project was my crash course in computer science, low-level programming, app development, electrical circuit theory, and app development. In hindsight, it was quite complicated for someone like me, who has never worked with embedded systems (actual hardware) before. . The project was expensive, definitely time consuming, incredibly frustrating at times, and yet may have been one of the most rewarding things I can remember doing. . Code for ESP32 Beacon Sensor and Neopixels | Code for Smart Home App | . Lessons . Beware - the following is long and fairly unstructured. But it may help someone - or at the very least it will be my online reference for later. . Microcomputing (mostly ESP32) . An ESP32 can be referred to as a SoC (System on a Chip) and/or an MCU (Microcontroller unit). However my understanding is that an MCU is appropriate when its just the ESP32 (no peripherals) whereas the SoC is for an ESP32 with peripherals (ie. a dev board). . Some older models of ESP32s require antennae. The first ESP32 I got required one - ESP-WROOM-32U. I saw what looked like a clip for the antenna but ignored it. . I spent many hours troubleshooting why this ESP32 got the RSSI (proximity) values were so off before I clued in that an antenna would be massively helpful. This discovery was a huge time sink. | In my case, consistent with other testaments online, using this device without an antenna actually ruined the device - even with one attached, nothing was accurate and Wifi connections were not reliable. A new ESP32 confirmed it was an antenna issue when RSSI. | . Reading the datasheets of any device or chip is helpful to understand the device as this provides the pinouts (mapping of pins to functions), the voltage inputs and outputs, etc. However, the following are rough guidelines for the ESP32 that I learnt: . The ESP32 runs on 3.3V All output GPIO pins are 3.3V | GPIO input pins will tolerate up to 3.7V | . | You can power the ESP32 3 (sometimes 4) ways Voltage regulated - an AMS1117 3.3V regulator transforms the inputs into the working 3.3V: 5V USB - standard micro USB | VIN pin - this method is a bit contentious on several forums. Some say that you actually need 7-12V (as 1-2V may be lost in heat), others say that 5V is fine. I can’t quite understand why 7-12V is suggested for this method but not for the USB connection, when both methods pass through the voltage regulator to output 3.3V. The one thing for sure is that the 5V power supply must be high quality that supplies consistent and stable 5V (unlike phone chargers). However some have suggested a buck converter could be placed in between the power supply and the VIN, but I imagine this is more useful if you are using a higher voltage power supply. I have learnt not all chargers are equal. USB phone chargers are rated for 5V but they actually fluctuate in their output. . | Voltage unregulated 3.3V pin - you can actually input voltage to this pin so long as it is very close to 3.3V, but in my case I need this output 3.3V anyway so this method isn’t an option. | Battery - I didn’t do much research on this method as this was never in the cards. | . | . | . | . The ESP32 chip consumes anywhere from 160-260mA in active mode. Note that the ESP32 has multiple power modes, but I am powering this through the mains (wall socket) so I am not concerned with these modes at the moment . The Makers World . Don’t be fooled by some posts that suggest this may be a cheap way to make smart lights. This has not been cheap. If you don’t have any maker equipment, then the amount of tools you need are substantial. . This isn’t to discourage - this has been one of the most rewarding projects I have done. It is an honest assessment that for me this has cost several hundred CAD more than using just the tools above because I started from scratch. One point to acknowledge however - I’ve come to the realization that this hasn’t exactly been environmentally friendly. A lot of travel carbon, packaging, and unknown device manufacturing standards . Electrical Theory . I could write a lot more here - but there are far better resources out there to explain this. However highlighted below are the key concepts I took away. . Voltage = Electrical Potential = Potential Difference between 2 points . Note that Potential is the same as Gravitaional Potential Energy - ie. when an object is at the edge of a cliff, there is the potential to do work, but only when invoked (pushed). . Ohm’s Law . If I took anything away from this project, it is the following: . Do not think of Ohm’s Law as . V=IRV=IRV=IR . but rather . I=V/RI=V/RI=V/R . Current flows from high voltage potential (ie. positive end of battery) to lower voltage potential (ground, 0 volts). This water-and-pipe analogy is massively helpful. . . Source: https://cdn.sparkfun.com/assets/6/f/b/5/3/5113d1c3ce395fcc7d000000.png . Note that online tutorials are far better at explaining it, but the key takeaway is that the water flow (current) is a function of the amount of water (pressure, voltage) and the width of the pipe (resistance). This way of thinking is exactly like framing Ohm’s law to ficus on current and not voltage. . Voltage Drop . Here is where theory became practice. Grasping electrical theory and Ohm’s Law is great, but another key element in building a circuit is understanding the materials you use and their impact. . For my project, I was planning to run a 2.2m length of cable between the device and the LEDs since I wanted the ESP32 out of the closet (for safety concerns). . I was made aware that there would be a voltage drop across the connecting wires due to its length, despite the fact that I used 18 AWG wires (see the Wire Gauge section). | . brightness = 50 total_brightness = 255 # FastLED uses a scale of 0-255 to set brightness brightness_ratio = brightness/total_brightness max_single_amperage = 60 #mA, typical for white Neopixel on max brightness num_leds = 68 estimated_amperage = brightness_ratio * max_single_amperage * num_leds estimated_amperage #800mA . To calculate voltage drop, we use the formula Vdrop=I∗RV_{drop} = I*RVdrop​=I∗R, where I is the current through the wire and R is the resistance of the wire (in reality, the resistance of a wire is impacted by many things, including the distance, width/gauge, whether the current is single or three-phased, AC or DC). Therefore I outsourced the calculation to this online calculator, the expected voltage drop at this low current would be 0.074V (or 1.47%), leaving 4.926V at the end. . In reality, the voltage drop was negligible. . If we were to increase the brightness of the 68 LEDs to half their theoretical max (125), thus increasing the current load, the voltage drop increases to 0.18V (or 3.69%), leaving only 4.82V. However according to my multimeter, the voltage dropped only 30mV . Note shortening the distance to 20cm reduces the voltage drop at 2A load to just 0.017V (0.34%). . This begs the question though - if all we are doing is deferring the distance traveled by the electrical current to an extension cord instead of the custom build cables, don’t extension cords also experience significant voltage drop and thus will also impact the LED colour? The answer is yes and no. We need to think about this in the context of the types of current each is transmitting (in addition to the significantly larger gauge that an extension cord has). Devices that plug directly into a wall outlet (ie. a lamp) are receiving 120V (in NA; 220-240V mostly elsewhere). Devices like a phone or computer charger - this is a misnomer, they are actually power adapters that just convert AC to DC and step down the voltage to say 5V - have transformers and other components that convert AC to DC and down to a usable voltage. So back to our extension cord vs jumper wire discussion - the extension cord transmits 15-20A AC at 120V, therefore any voltage drop will be negligible (both due to the high voltage and the smaller gauge). Given that our 5V power source transforms any voltage over 5V down to 5V, it doesn’t matter if the extension cord experienced voltage drop at least for our use case. So the answer therefore is if we had to choose, we would use extension cords to bring the 120V source as close to the LEDs, so that the subsequent DC output jumper cables can be as short as possible and thus experience the least amount of voltage drop when powering the LEDs. Most NA house circuits have 15 or 20A, which equates to (15A * 120V =) 1800 watts or (20A * 120V =) 2400 watts before the breaker trips (source) . Why do we care about voltage drop for these LEDs? . If the LEDs don’t get close to the 5V they expect, the colours start to look whitewashed and saturated. This effect becomes more pronounced the further down the LED strip you go (because WS2812B pixels strips are connected in series, and each LED has its own resistance, thus creating an additive loss of current as you move down the strip). . Note most forums I read suggest adding multiple power supply connections along a strip of LEDs (roughly one connection for every metre). However since my strip is only 1m long, I am not too concerned. . It has also come to my attention that we step down (transform) voltages in every device. Where does the excess power go? The first law of thermodynamics states that since we can’t create or destroy energy, the difference in power from what the device consumes and what it was provided is converted into some other form of energy. In most cases this new energy form is heat - in fact this is what purpose-built resistors do - they convert a certain amount of electrical energy into heat energy. Surely this system, despite how good it has worked for humanity thus far, is quite inefficient? . Power &amp; Energy . Power is the amount of energy transferred or converted per unit time. Its is a rate of how much energy can do work (ie. how much work can be done at a single moment) . P=VIP = VIP=VI . The unit of Power is Watt (W) | 1 W = 1 Joule/second | Interestingly: 1HP=746W1 HP = 746W1HP=746W | . Energy on the other hand is the total amount of work done over a period of time - it is power over time. . E=P∗tE = P*tE=P∗t . The unit of Energy is the Joule, which based on the definition above is 1W per second | It is usually measured in kiloWatt hours (kWh) on our hydro bill | 1 V = 1 J of energy done by 1 coulomb of charge A coulomb is just a defined number of electrons, which for our purposes is a large arbitrary number (like a mole in chemistry) | . | . This is the best explanation of power vs energy. Lifting the box in the image below requires a specific amount of energy, no matter how quickly the box is picked up. Lifting faster will change the amount of power, but not the amount of energy. . . Source: https://energyeducation.ca/wiki/images/2/2b/Lifting_a_box.jpg . Wire Gauge . Wire gauge is quite important, and this is one area that adds to the cost unexpectedly for a newbie. It also is the point at which the simplified electrical theory understood thus far starts to break down, and we realize that this field is in reality far more complex. Wire gauge (diameter) impacts the circuits ability to carry current over longer distances. . The higher the gauge, the thinner the wire | The lower the gauge, the higher its current carrying capacity and the less voltage drop that will occur over longer distances. This was important for my project where the distance between the power supply and the LED strip was 2.2m. | According to Wires, connectors and current - what you need to know as a drone builder - Guides - DroneTrest, an 18 AWG wire can carry up to 30A, whereas 22 AWG is limited to 10A. | . The general formula for calculating the max amperage for a wire is: . Amps=cross sectional area (mm2)×25Amps = text{cross sectional area } (mm^2) times 25Amps=cross sectional area (mm2)×25 . Resistance per metre: . Resistance per meter=0.0168/ cross sectional area (0.0168/mm2) text{Resistance per meter} = 0.0168 / text{ cross sectional area } (0.0168 / mm^2)Resistance per meter=0.0168/ cross sectional area (0.0168/mm2) «««&lt; HEAD:_posts/2021-10-30-proximity_leds_lessons.md . Voltage drop per metre . Voltage drop per metre: . 9bd14e895c18cc3f9d71953ca3787165f2a68a24:_posts/2021-10-30-smarthome_base.md . Voltage drop per meter=(current×0.0168)/ cross sectional area ((currentx0.0168)/mm2) text{Voltage drop per meter} = (current times 0.0168)/ text{ cross sectional area } ((current x 0.0168) / mm^2)Voltage drop per meter=(current×0.0168)/ cross sectional area ((currentx0.0168)/mm2) . Power loss per metre: . Power loss per meter=(current2)×0.0168)/ cross sectional area ((current2)×0.0168)/mm2) text{Power loss per meter} = (current^2) times 0.0168)/ text{ cross sectional area } ((current^2) times 0.0168) / mm^2)Power loss per meter=(current2)×0.0168)/ cross sectional area ((current2)×0.0168)/mm2) . Connecting Wires Together . Keep in mind that breadboarding is a great start - however in order to create an actual device that is safe and portable, you need to eventually connect things together. For wires, I found that it was hard to commit to soldering them directly to endpoints. Therefore I often went with connectors - much easier to correct mistakes and adjust the device as you iterate (seeing as I was not working off a blueprint - the chances of mistakes were high). . There are several different types of connectors. I just happened to blindly purchase JST connectors off Amazon. These JST connectors have on average 0.04V drop per amp of current according to JST connector volage drop measurement results. - RC Groups. Therefore we should always solder when possible. There is very little voltage loss through a solder joint. To be more specific, solder still has 5x the resistance as copper, so the most ideal answer is a solid copper connection. However this is never really possible . All this being said, small circuits that don’t require significant amperage (ie. &lt;1A) can use the standard 22AWG for breadboards. And despite breadboards having high voltage loss do it its small/poor quality contacts, this is normally fine for prototyping low power circuits. However it is important to note that breadboards generally have a current limit of 1A . Solderable breadboards (often called protoboards or perma-protoboards) have a much higher amperage. | Even though the embedded copper connectors in a protoboard are far superior to breadboards (and can handle much higher currents), most still say to err on the side of caution and add an additional wire to the high current traces of the circuit. | . Soldering 18AWG wire to the small pads of the LEDs was very tough. Eventually I realized that trimming/filing one side of the exposed 18AWG wire so that it was flat against the LED copper pads was the trick that worked. However using JST connectors for wire of this low gauge is not the best idea for the next project. . After all of this work, it is clear that there is inevitable voltage drop that will occur the circuit contains the following characteristics: . Current travelling long distance | Number of connections (meaning number of joints such as solders, crimps) | Smaller wire gauge | More resistors in the circuit (everything is a resistor, therefore the more complex the circuit the higher the voltage drop) | . Some design changes I would consider if I were to restart this project: . Use a higher voltage power source (6 ot 7V) and use buck converters (weird name I know - they are configurable devices that allow you to drop voltage to a certain level) to lower the voltage near the devices (in my case near the LEDs and the logic level signal converter; the ESP32 could handle a higher voltage than 5V if using the VIN) A higher voltage does two things (although they may be dependent of eachother): Reduces voltage loss (important over longer distances) | Creates voltage buffer so that any voltage drop still results in voltage at or above the device requirement | . | . | . Circuit Design . It had been a while since I had thought about circuits in series and in parallel. . Series Circuit: resistances in series are additive Parallel Circuit: the potential difference across any component is the same and equal to the supply voltage. . So in this new design, if we have two wires in parallel - 1 feeding the protoboard containing the ESP32, and one powering the lights directly - what is the current distribution across the two wires? . Well we know that an object’s resistance is (basically) constant. Since energy cannot be created or destroyed, and voltage and resistance are constant, with Ohm’s Law we can deduce that the current will halve between the two wires (assuming their resistances are the same). . Prototyping . The initial testing and design was done using a standard breadboard. In reading some of the forums discussing Neopixels, I was made aware that sending high amounts of current through a breadboard is really discouraged (and even dangerous). The estimated max current (ampacity) through a regular breadboard seems to be around 1A. . Protoboard . A solderable breadboard with embedded channels (wires) is the better approach to prototype once you have designed and tested the initial layout. . It was difficult to find an answer on the max ampacity for a protoboard. According to this old forum Adafruit customer service forums • View topic - Perma-proto board ampacity, the ptotoboards power lines are 32 mil wide. When using this trace width calculator to estimate the voltage drop and heat produced, it suggested that for a 5C temperature increase I would only see a 0.05V drop (thereby suggesting this setup was relatively safe?) . ![[Pasted image 20211016204955.png]] . C++ . Because I chose to use the Arduino (a C++ derivative) language instead of Python, I had to get up to speed with C++ (something that has proven very difficult, but quite rewarding). Below are some of my oberservations as I tried to pick up the language: . Not much different to Dart from what I can tell (this is probably offensive to those that know the langueages inside and out) | I have found the elements that are tied deep to computer science theory to be the most difficult (things that are managed for you in Python and even Dart): Pointers and references. Not so much what they are, but when to use them. | Arrays in c++. I just don’t understand them. | . | Some may cringe at me calling development on the ESP32 c++ when in reality it is Arduino code. But for my purpose they are the same. And one of the advantages of using the Arduino framework is that the libraries are well documented and purpose built for SoC’s like the ESP32. | Some of the other c++ habits I have developed that help me in my other coding (especially Dart) are: Type declarations: not always needed in Dart, but required in c++, this helps you to think about how the object will sit in memory; the whole point of type declaration is that we are letting the computer know how many bytes this object will need allocated in memory. | const: should be used when creating variables wherever possible as it is the most performative. The reason is its value is calculated at compile time, meaning unlike other variables, the program doesn’t have to go searching for what its value is when running. Note final means something different in Dart vs c++ | . | Stack vs Heap: Stack: temporary memory allocation that allocates and deallocates memory automatically; safe - data on stack only accessible by the owner thread - and faster to read/write. Whenever a function is called, its variables are placed on the stack and are only available while the function is running. See below from Stack vs Heap Memory Allocation - GeeksforGeeks: | . | . int main() { // All these variables get memory // allocated on stack int a; int b[10]; int n = 20; int c[n]; } . Heap: simply opposite of the stack. | . | . | . FastLED &amp; WS2812B . I used the FastLED Arduino library to configure the LEDs since they had pre-programmed light sequences, which I didn’t feel like piecing together. Again, some of my notes are below: . Colours were off and I still can’t explain why. However I do see all of the colours represented when I run a rainbow script - therefore, there is a | These series of Youtube videos were excellent and they explained some of the basics of colour theory The one thing I took away is that the first number in the Hue colour system is a continuous 0-255 rolling | . | . Some notes on setting up the WS2812B LEDs: . You CANNOT attach the data wire in any place other than the start of a strip. This caused many hours of troubleshooting, as I had soldered the wires to the second LED in the string as the first one’s copper pads were too small for the 18 AWG wire. However once I clipped off the first LED, the programme worked | The data pulse output from the GPIO pin to the LEDs is extremely fast - so fast that my cheap multimeter could not process the output before it “disappeared”. This also compounded my troubleshooting issue mentioned above as it looked like my program wasn’t working correctly (since when testing the GPIO output, the multimeter wasn’t reading any voltage) when in reality the issue was in the physical circuit. I assume that since we are powering the LED strips with external power, the data wire simply set which LEDs are to turn on via a quick pulse, but the sustained power is provided by the external source and thus the data output is only needed for an extremely quick pulse. | . | . Flutter App . The setup of Flutter is actually straight forward. The two elements of the app development process that were the most tricky were: . State management: used Riverpod for this. Its a fantastic library and I really respect its developer Remi, but it has a steep learnign curve (for me anyway) | Deployment for testing: I used Firebase app distribution, alongside XX and Github actions to always push the latest version of the app to my phone for testing. This worked with varying degrees of success | Finding library(ies) that were permitted to run in the smart phone’s background. Understandably, iOS and Android restrict what a developer can run and access on a user’s phone to avoid , but these restrictions were quite constraining for an app I only ever intended for myself and my wife. | . Bluetooth . I decided to use Bluetooth Low Energy as the beacon mechanism on my phone, and take the resulting Received Signal Strength Indicator (RSSI) to gauge how close the phone is to the ESP32. I understand that RSSI is an imperfect localization indicator, but until I have a phone and SoC with Bluetooth 5+ or ultra wideband (UWB) technology, this is the best option available to me. . Note that I got some help from this reddit post, which also references a cool cat localizer project. . Next Steps . The next blog will run through the physical setup updates of the lights to make them extra safe (and allow for them to be brighter). . Upcoming blogs will also: . Review the Flutter app (incl. describing the lessons learnt from using Riverpod state management library) and turn off advertising when leaving the geofence | Update of the ESP32 code . | Allow for colour changes via app | Integrate Google Assistant to turn on/off lights | Power ESP32 via 5V power adapter Build switch to stop 5V input to VIN so that we can still connect USB if needed (remember they can’t both be connected at the same time) | . | Build redundancy and failsafe mechanisms Backup RPi server | Capture errors: Node Red flows | MQTT down (ie. maybe if MQTT is down, we revert back to just turning the lights on?) | . | . | Phone beacon battery optimization | . Personal Development Outcomes . I can write a bit of c++ code | I can write a bit of Dart/Flutter code | I can use Github Actions and fastlane | I know a lot more about electricity (and thus how the modern world operates) | I can solder | I can use MQTT and Node-Red | I know more about Docker containers | .",
            "url": "https://stantonius.github.io/home/iot/microcomputing/flutter/2021/10/30/proximity_leds_lessons.html",
            "relUrl": "/iot/microcomputing/flutter/2021/10/30/proximity_leds_lessons.html",
            "date": " • Oct 30, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://stantonius.github.io/home/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://stantonius.github.io/home/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Thanks for checking out the site. . My name is Craig Stanton and I am a self-taught developer based in Ottawa, Canada. I have a wide array of technology interests, which seem to change almost daily. Currently I spend a lot of time on IoT and AI projects, but this blog may contain content from a wider variety of domains. . Despite being the opposite in person, I am extremely shy and reserved when it comes to my online profile and presence. This blog is an attempt to break out of this shell and mindset. I have finally come to realize that this online lurking behaviour is quite selfish - a lot of my learnings have come from very kind people posting online, and despite my phobia of doing the same, I need to contribute in some capacity. . So if you read this and are like me, or have similar interests to me, feel free to reach out and connect or comment on any of the posts on this blog. I like to think I am a stereotypically friendly Canadian who is always open to meet and learn from new people. .",
          "url": "https://stantonius.github.io/home/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stantonius.github.io/home/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}